---
title: "zhzh"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


```{python}
from bs4 import BeautifulSoup
import requests
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}

url = "https://oig.hhs.gov/fraud/enforcement/"

response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

titles = []
dates = []
categories = []
links = []

for action in soup.select('li.usa-card.card--list.pep-card--minimal'):
    title_tag = action.select_one('h2.usa-card__heading a')
    title = title_tag.get_text(strip=True)
    link = f"https://oig.hhs.gov{title_tag['href']}"

    date = action.select_one('span.text-base-dark').get_text(strip=True)
    category = action.select_one('li.usa-tag').get_text(strip=True)
    
    titles.append(title)
    dates.append(date)
    categories.append(category)
    links.append(link)

data = {
    "Title": titles,
    "Date": dates,
    "Category": categories,
    "Link": links
}
df = pd.DataFrame(data)

df
```

  
### 2. Crawling (PARTNER 1)

```{python}
titles = []
dates = []
categories = []
links = []
agencies = []
for action in soup.select('li.usa-card.card--list.pep-card--minimal'):
    title_tag = action.select_one('h2.usa-card__heading a')
    title = title_tag.get_text(strip=True)
    link = f"https://oig.hhs.gov{title_tag['href']}"
    
    date = action.select_one('span.text-base-dark').get_text(strip=True)
    category = action.select_one('li.usa-tag').get_text(strip=True)
    
    titles.append(title)
    dates.append(date)
    categories.append(category)
    links.append(link)
    
    try:
        action_response = requests.get(link)
        action_soup = BeautifulSoup(action_response.text, 'html.parser')
        
        agency_name = "Not Found" 
        for label in action_soup.find_all('span'):
            if "Agency:" in label.get_text():
                agency_name = label.find_next_sibling(text=True).strip() if label.find_next_sibling(text=True) else "Not Found"
                break  
    except Exception as e:
        agency_name = "Not Found" 
    
    agencies.append(agency_name)

min_length = min(len(titles), len(dates), len(categories), len(links), len(agencies))
data = {
    "Title": titles[:min_length],
    "Date": dates[:min_length],
    "Category": categories[:min_length],
    "Link": links[:min_length],
    "Agency": agencies[:min_length]
}

df = pd.DataFrame(data)

df
```


## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
1. Input Validation:
Check if the year is greater than or equal to 2013. If the year is less than 2013, print a reminder to restrict the year to >= 2013.

2. URL Construction:
Based on the input month and year, construct the starting URL for scraping (e.g., page 1, page 2, etc.).
Loop through multiple pages to gather all the data.

3. Scraping and Storing Data:
Scrape the enforcement actions from each page (titles, dates, categories, links, agencies).
Store the scraped data in lists.
After scraping all pages, save the data into a DataFrame.

4. Save to CSV:
After scraping all enforcement actions, save the data to a .csv file named enforcement_actions_year_month.csv.

* b. Create Dynamic Scraper (PARTNER 2)


```{python}
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import nest_asyncio

nest_asyncio.apply()

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def fetch_agency(session, link):
    """Fetches the agency name from the action detail page."""
    try:
        html = await fetch(session, link)
        soup = BeautifulSoup(html, 'html.parser')
        
        agency_name = "Not Found"
        for label in soup.find_all('span'):
            if "Agency:" in label.get_text():
                agency_name = label.find_next_sibling(text=True).strip() if label.find_next_sibling(text=True) else "Not Found"
                break
        return agency_name
    except Exception as e:
        print(f"Error fetching agency for {link}: {e}")
        return "Not Found"

async def scrape_page(session, page_number, start_date, titles, dates, categories, links, agencies):
    url = f"https://oig.hhs.gov/fraud/enforcement/?page={page_number}"
    print(f"Scraping page {page_number}...")

    html = await fetch(session, url)
    soup = BeautifulSoup(html, 'html.parser')
    actions = soup.select('li.usa-card.card--list.pep-card--minimal')

    if not actions:
        print(f"No actions found on page {page_number}.")
        return False  

    page_reached_start_date = False

    for action in actions:
        title_tag = action.select_one('h2.usa-card__heading a')
        title = title_tag.get_text(strip=True)
        link = f"https://oig.hhs.gov{title_tag['href']}"
        
        date_str = action.select_one('span.text-base-dark').get_text(strip=True)
        action_date = datetime.strptime(date_str, "%B %d, %Y")

        if action_date < start_date:
            page_reached_start_date = True
            break  # Stop processing further actions on this page if before start_date

        category = action.select_one('li.usa-tag').get_text(strip=True)
        
        titles.append(title)
        dates.append(date_str)
        categories.append(category)
        links.append(link)

        agency_name = await fetch_agency(session, link)
        agencies.append(agency_name)

    return not page_reached_start_date

async def scrape_enforcement_actions(year, month, max_pages=480, batch_size=10):
    start_date = datetime(year, month, 1)
    titles, dates, categories, links, agencies = [], [], [], [], []

    async with aiohttp.ClientSession() as session:
        for start_page in range(1, max_pages + 1, batch_size):
            tasks = [
                scrape_page(session, page_number, start_date, titles, dates, categories, links, agencies)
                for page_number in range(start_page, min(start_page + batch_size, max_pages + 1))
            ]

            results = await asyncio.gather(*tasks)
            if not all(results): 
                print("Stopping scraping as reached entries before start_date.")
                break

    data = {
        "Title": titles,
        "Date": dates,
        "Category": categories,
        "Link": links,
        "Agency": agencies
    }
    df = pd.DataFrame(data)
    csv_filename = f"enforcement_actions_{year}_{month}.csv"
    df.to_csv(csv_filename, index=False)

    print(f"Data saved to {csv_filename}")
    print(f"Total records: {len(df)}")
    print(f"Earliest date in data: {df['Date'].iloc[-1] if not df.empty else 'No data'}")
    return df

year, month = 2023, 1
await scrape_enforcement_actions(year, month)

```

There are 1510 records that I got. The earliest date in data was on Feb 7, 2023.
Twenty-Three Individuals Charged In $61.5 Mill...	February 7, 2023	Criminal and Civil Actions	https://oig.hhs.gov/fraud/enforcement/twenty-t...	U.S. Department of Justice.
  
* c. Test Partner's Code (PARTNER 1)

```{python}
# Set the start date to January 2021
year, month = 2021, 1
await scrape_enforcement_actions(year, month, batch_size=50)
```

There are 2998 records that I got. The earliest date in data was on September 17, 2021. 
Gloucester County Man Charged with Fraud for R...	September 17, 2021	Criminal and Civil Actions	https://oig.hhs.gov/fraud/enforcement/gloucest...


## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
import pandas as pd
import altair as alt

df_jan_2021['Date'] = pd.to_datetime(df_jan_2021['Date'])
df_jan_2021['Year_Month'] = df_jan_2021['Date'].dt.to_period('M')

monthly_counts = df_jan_2021.groupby('Year_Month').size().reset_index(name='Count')
monthly_counts['Year_Month'] = monthly_counts['Year_Month'].dt.to_timestamp()

line_chart = alt.Chart(monthly_counts).mark_line().encode(
    x=alt.X('Year_Month:T', title='Date'),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions'),
    tooltip=['Year_Month:T', 'Count:Q']
).properties(
    title='Number of Enforcement Actions Over Time (Aggregated by Month and Year)',
    width=600,
    height=400
)

line_chart.show()
```

![Number of Enforcement Actions Over time](chart1.png)

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
df_jan_2021['Date'] = pd.to_datetime(df_jan_2021['Date'])

df_jan_2021['Year_Month'] = df_jan_2021['Date'].dt.to_period('M')

monthly_counts = df_jan_2021.groupby(['Year_Month', 'Category']).size().reset_index(name='Count')
monthly_counts['Year_Month'] = monthly_counts['Year_Month'].dt.to_timestamp()  

monthly_counts = monthly_counts[monthly_counts['Category'].isin(["Criminal and Civil Actions", "State Enforcement Agencies"])]

line_chart = alt.Chart(monthly_counts).mark_line().encode(
    x=alt.X('Year_Month:T', title='Date'),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions'),
    color=alt.Color('Category:N', title='Category'),
    tooltip=['Year_Month:T', 'Category:N', 'Count:Q']
).properties(
    title='Number of Enforcement Actions Over Time by Category',
    width=700,
    height=400
)

line_chart
```

![criminal vs. enforcement](chart2.png)

* based on five topics

```{python}
def classify_topic(title):
    """Classifies each action title into one of the five topics or 'State Enforcement Agencies'."""
    title = title.lower()
    if "health" in title or "care" in title:
        return "Health Care Fraud"
    elif "financial" in title or "bank" in title or "money" in title:
        return "Financial Fraud"
    elif "drug" in title or "narcotics" in title:
        return "Drug Enforcement"
    elif "bribery" in title or "corruption" in title or "bribe" in title:
        return "Bribery/Corruption"
    else:
        return "Other"

df_jan_2021['Topic'] = df_jan_2021.apply(
    lambda row: classify_topic(row['Title']) if row['Category'] == "Criminal and Civil Actions" else "State Enforcement Agencies", 
    axis=1
)

monthly_counts = df_jan_2021.groupby(['Year_Month', 'Topic']).size().reset_index(name='Count')
monthly_counts['Year_Month'] = monthly_counts['Year_Month'].dt.to_timestamp()

line_chart = alt.Chart(monthly_counts).mark_line().encode(
    x=alt.X('Year_Month:T', title='Date'),
    y=alt.Y('Count:Q', title='Number of Enforcement Actions'),
    color=alt.Color('Topic:N', title='Topic', scale=alt.Scale(domain=[
        "Health Care Fraud", "Financial Fraud", "Drug Enforcement", 
        "Bribery/Corruption", "Other", "State Enforcement Agencies"
    ])),
    tooltip=['Year_Month:T', 'Topic:N', 'Count:Q']
).properties(
    title='Number of Enforcement Actions Over Time by Topic',
    width=700,
    height=400
)

line_chart
```

![five topics](chart3.png)

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

enforcement_data = pd.read_csv('enforcement_actions_2023_1.csv')

state_shapefile_path = 'cb_2018_us_state_5m.shp'
states = gpd.read_file(state_shapefile_path)

```


```{python}
state_actions = enforcement_data[enforcement_data['Agency'].str.contains("State of", na=False)]

state_actions['State'] = state_actions['Agency'].str.extract(r"State of (\w+)")
state_actions['State'] = state_actions['State'].str.strip()  

state_counts = state_actions['State'].value_counts().reset_index()
state_counts.columns = ['State', 'Enforcement_Count']

state_choropleth = states.merge(state_counts, how="left", left_on="NAME", right_on="State")

plt.figure(figsize=(15, 10))
state_choropleth.plot(column='Enforcement_Count', cmap='OrRd', legend=True, edgecolor="black")
plt.title("State-Level Enforcement Actions by State")
plt.axis("off")
plt.show()
```

![map by state](graph1.png)

### 2. Map by District (PARTNER 2)

```{python}
district_shapefile_path = 'geo_export_fcd06d4e-838a-449a-979d-dfc51a522ff4.shp'
district= gpd.read_file(district_shapefile_path)
print(district.columns)
```

```{python}
enforcement_data = pd.read_csv('enforcement_actions_2023_1.csv')
print(enforcement_data.columns)

enforcement_data = pd.read_csv('enforcement_actions_2023_1.csv')

# Filter for entries that mention "District" to isolate district-level actions
district_actions = enforcement_data[enforcement_data['Agency'].str.contains("District", na=False)]

# Extract and clean district names from 'Agency' (assuming 'District of <Name>' or similar format)
# Adjust regex as necessary to capture other naming variations in your data
district_actions['District'] = district_actions['Agency'].str.extract(r'(.*District of .+?)')[0]
district_actions['District'] = district_actions['District'].str.strip()

# Count the number of enforcement actions by district
district_counts = district_actions['District'].value_counts().reset_index()
district_counts.columns = ['District', 'Enforcement_Count']

# Standardize district names to match shapefile (e.g., removing extra spaces or characters)
district_counts['District'] = district_counts['District'].str.replace(r'\s+', ' ', regex=True).str.strip()
district['judicial_d'] = district['judicial_d'].str.replace(r'\s+', ' ', regex=True).str.strip()

# Merge the cleaned counts with the district shapefile using 'judicial_d'
district_choropleth = district.merge(district_counts, how="left", left_on="judicial_d", right_on="District")

# Plot the choropleth map
plt.figure(figsize=(15, 10))
district_choropleth.plot(column='Enforcement_Count', cmap='Blues', legend=True, edgecolor="black")
plt.title("US Attorney District-Level Enforcement Actions")
plt.axis("off")
plt.show()

```

```{python}
print(district_counts['District'].unique())
print(district['judicial_d'].unique())

```

```{python}
print(district_choropleth[['judicial_d', 'Enforcement_Count']].head())
print(district_choropleth['Enforcement_Count'].isna().sum())
```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```